---
title: "Correlation and Regression"
author: "Camila Bezerra"
date: "03/09/2019"
output:
  html_document:
    theme: united
    highlight: monochrome
---

```{r, warning=F}
library(ggplot2)
library(tidyverse)
library(dplyr)
```


# 1. Visualizing two variables

## Visualizing bivariate relationships

Using the ncbirths dataset again, make a boxplot illustrating how the birth weight of these babies varies according to the number of weeks of gestation. This time, use the cut() function to discretize the x-variable into six intervals (i.e. five breaks).

```{r, eval = F}
# Boxplot of weight vs. weeks
ggplot(data = ncbirths, 
       aes(x = cut(weeks, breaks = 5), y = weight)) + 
  geom_boxplot()
```

## Characterizing bivariate relationships

* Form(e.g. linear, quadratic, non-linear)
* Direction(e.g. positive, negative)
* Strength(how mucj scatter/noise?)
* Outliers

## Transformations

The coord_trans() function transforms the coordinates of the plot. Alternatively, the scale_x_log10() and scale_y_log10() functions perform a base-10 log transformation of each axis. Note the differences in the appearance of the axes.

```{r, eval= FALSE}
# Scatterplot with coord_trans()
ggplot(data = mammals, aes(x = BodyWt, y = BrainWt)) +
  geom_point() + 
  coord_trans(x = "log10", y = "log10")

## OR

# Scatterplot with scale_x_log10() and scale_y_log10()
ggplot(data = mammals, aes(x = BodyWt, y = BrainWt)) +
  geom_point() +
  scale_x_log10()+ 
  scale_y_log10()
```

## outliers

### Add transparency

```{r}
ggplot(data = mlbBat10, aes(x = SB, y = HR))+
  geom_point(alpha = 0.5)
```

### Add some jitter

```{r}
ggplot(data = mlbBat10, aes(x = SB, y = HR))+
  geom_point(alpha = 0.5, position = "jitter")
```

### Identify the outliers

```{r}
mlbBat10 %>% 
  filter(SB > 60 | HR > 50) %>% 
  select(name, team, position, SB, HR)
```

# 2. Correlation

## Computing correlation

The cor() function is very conservative. Setting use argument to "pairwise.complete.obs" allows cor() to compute the correlation coefficient for those observations where the values of x and y are both missing.

```{r}
# Compute correlation
ncbirths %>%
  summarize(N = n(), r = cor(weight, mage))

# Compute correlation for all non-missing pairs
ncbirths %>%
  summarize(N = n(), r = cor(weight, weeks, use = "pairwise.complete.obs"))
```

# 3. Simple linear regression

##Fitting a linear model "by hand"

Recall the simple linear regression model:
Y=b0+b1⋅X
Two facts enable you to compute the slope b1 and intercept b0 of a simple linear regression model from some basic summary statistics.

First, the slope can be defined as:

b1=rX,Y⋅sYsX
where rX,Y represents the correlation (cor()) of X and Y and sX and sY represent the standard deviation (sd()) of X and Y, respectively.

Second, the point (x¯,y¯) is always on the least squares regression line, where x¯ and y¯ denote the average of x and y, respectively.

The bdims_summary data frame contains all of the information you need to compute the slope and intercept of the least squares regression line for body weight (Y) as a function of height (X). You might need to do some algebra to solve for b0!

###Hint

To get intercept, first calculate slope, then solve
Y=b0+b1⋅X
 
for  b0 . Finally, plug in mean_wgt for  Y , mean_hgt for  X , and slope for  b1 .

```{r, eval = F}
# Print bdims_summary
bdims_summary

# Add slope and intercept
bdims_summary %>%
  mutate(slope = r * sd_wgt/sd_hgt, 
         intercept = mean_wgt - slope * mean_hgt)
```

## Regression vs. regression to the mean

### Heredity

* Galton's regression to the mean

The basic idea is that extreme random observations will tend to be less extreme upon a second trial. This is simply due to chance alone. While "regression to the mean" and "linear regression" are not the same thing, we will examine them together in this exercise.

One way to see the effects of regression to the mean is to compare the heights of parents to their children's heights. While it is true that tall mothers and fathers tend to have tall children, those children tend to be less tall than their parents, relative to average. That is, fathers who are 3 inches taller than the average father tend to have children who may be taller than average, but by less than 3 inches.

```{r, eval=F}
#Create a scatterplot of the height of men as a function of their father's height. Add the simple linear regression line and a diagonal line (with slope equal to 1 and intercept equal to 0) to the plot.
# Height of children vs. height of father
ggplot(data = Galton_men, aes(x = father, y = height)) +
  geom_point() + 
  geom_abline(slope = 1, intercept = 0) + 
  geom_smooth(method = "lm", se = FALSE)

## Create a scatterplot of the height of women as a function of their mother's height. Add the simple linear regression line and a diagonal line to the plot.
ggplot(data = Galton_women, aes(x = mother, y = height)) +
geom_point()+
geom_abline(slope = 1, intercept = 0)+
geom_smooth(method= "lm", se = FALSE)
```

Because the slope of the regression line is smaller than 1 (the slope of the diagonal line) for both males and females, we can verify Sir Francis Galton's regression to the mean concept!

Because of regression to the mean, and outstanding basketball plyaer is likely to have sons that are good at basketball, but not as good as him.

# 4. Interpreting regression models

Units and scale
In the previous examples, we fit two regression models:
wgtˆ=−105.011+1.018⋅hgt
and
SLGˆ=0.009+1.110⋅OBP.

Because the slope coefficient for OBP is larger (1.110) than the slope coefficient for hgt (1.018), we can conclude that the association between OBP and SLG is stronger than the association between height and weight.

##The lm summary output

An "lm" object contains a host of information about the regression model that you fit. There are various ways of extracting different pieces of information.

The coef() function displays only the values of the coefficients. Conversely, the summary() function displays not only that information, but a bunch of other information, including the associated standard error and p-value for each coefficient, the R2, adjusted R2, and the residual standard error. The summary of an "lm" object in R is very similar to the output you would see in other statistical computing environments (e.g. Stata, SPSS, etc.)

```{r, eval=FALSE}
# Show the coefficients
coef(mod)

# Show the full output
summary(mod)
```

## Fitted values and residuals
Once you have fit a regression model, you are often interested in the fitted values (y^i) and the residuals (ei), where i indexes the observations. Recall that:

ei=yi−y^i
The least squares fitting procedure guarantees that the mean of the residuals is zero (n.b., numerical instability may result in the computed values not being exactly zero). At the same time, the mean of the fitted values must equal the mean of the response variable.

```{r, eval=FALSE}
# Mean of weights equal to mean of fitted values?
mean(mod) == mean(fitted.values(mod))

# Mean of the residuals
mean(residuals(mod))
```

##Tidying your linear model
As you fit a regression model, there are some quantities (e.g. R2) that apply to the model as a whole, while others apply to each observation (e.g. y^i). If there are several of these per-observation quantities, it is sometimes convenient to attach them to the original data as new variables.

The augment() function from the broom package does exactly this. It takes a model object as an argument and returns a data frame that contains the data on which the model was fit, along with several quantities specific to the regression model, including the fitted values, residuals, leverage scores, and standardized residuals.

```{r}
# Load broom
library(broom)

# Create bdims_tidy
bdims_tidy <- augment(mod)

# Glimpse the resulting data frame
glimpse(bdims_tidy)
```

##Making predictions
The fitted.values() function or the augment()-ed data frame provides us with the fitted values for the observations that were in the original data. However, once we have fit the model, we may want to compute expected values for observations that were not present in the data on which the model was fit. These types of predictions are called out-of-sample.

The ben data frame contains a height and weight observation for one person. The mod object contains the fitted model for weight as a function of height for the observations in the bdims dataset. We can use the predict() function to generate expected values for the weight of new individuals. We must pass the data frame of new observations through the newdata argument.

```{r}
# Print ben
ben

# Predict the weight of ben
predict(mod, ben)
```

##Adding a regression line to a plot manually

The geom_smooth() function makes it easy to add a simple linear regression line to a scatterplot of the corresponding variables. And in fact, there are more complicated regression models that can be visualized in the data space with geom_smooth(). However, there may still be times when we will want to add regression lines to our scatterplot manually. To do this, we will use the geom_abline() function, which takes slope and intercept arguments. Naturally, we have to compute those values ahead of time, but we already saw how to do this (e.g. using coef()).

The coefs data frame contains the model estimates retrieved from coef(). Passing this to geom_abline() as the data argument will enable you to draw a straight line on your scatterplot.

# 5. Model Fit

## Assessing model fit

### SSE

```{r, eval = F}
library(broom)
mod_possum <- lm(totalL ~tail, data = possum)
mod_possum %>% 
  summarize(SEE = sum(.resid^2),
            SEE_also = (n()-1) * var(.resid))
```

## RMSE

The residual standard error reported for the regression model for poverty rate of U.S. counties in terms of high school graduation rate is 4.67. What does this mean?

* The typical difference between the observed poverty rate and the poverty rate predicted by the model is about 4.67 percentage points.

## Standard error of residuals

To make this estimate unbiased, you have to divide the sum of the squared residuals by the degrees of freedom in the model. Thus,

RMSE= \sqrt{\frac{\sum_ie^2} {df}} = \sqrt{\frac{SSE}{df}}
You can recover the residuals from mod with residuals(), and the degrees of freedom with df.residual().

```{r, eval=FALSE}
#View summary of model
summary(mod)

# Compute the mean of the residuals
mean(residuals(mod))

# Compute RMSE
sqrt(sum(residuals(mod)^2 / df.residual(mod)))
```

## Comparing model fits

### Null(average) model

* For all observations

$$\hat{y} = \overline{y}$$
### SSE, null model
```{r}
mod_null <- lm(total ~ 1, data = possum)
mod_null %>% 
  augment(possum) %>% 
  summarize(SST = sum(.resid^2))
```

### SSE, our model
```{r}
mod_possum <- lm(total ~tail, data = possum)
mod_possum %>% 
  augment() %>% 
  summarize(SSE = sum(.resid^2))
```


## Assessing simple linear model fit

Recall that the coefficient of determination (R2), can be computed as

$$R^2 = 1 - \frac{SSE}{SST} = 1 - \frac{Var(e)}{Var(y)}$$
e = vector of the residuals
y = response variable
R^2 = percentage of the variability in the response that is explained by the model, since the residuals are the part of that variability that remains unexplained by the model.

```{r}
# View model summary
summary(mod)

# Compute R-squared
bdims_tidy %>%
  summarize(var_y = var(wgt), var_e = var(residuals(mod))) %>%
  mutate(R_squared = 1 - var_e/var_y)
```

## Linear vs. average

The R^2 gives us a numerical measurement of the strength of fit relative to a null model based on the average of the response variable:

$$ \hat{y} \tiny{null} = \normalsize \overline{y} $$

```{r}
# Compute SSE for null model
mod_null %>%
  summarize(SSE = sum(var(.resid)))

# Compute SSE for regression model
mod_hgt %>%
  summarize(SSE = sum(var(.resid)))
```

##Leverage
The leverage of an observation in a regression model is defined entirely in terms of the distance of that observation from the mean of the explanatory variable. That is, observations close to the mean of the explanatory variable have low leverage, while observations far from the mean of the explanatory variable have high leverage. Points of high leverage may or may not be influential.

```{r}
# Rank points of high leverage
mod %>%
  augment() %>%
  arrange(desc(.hat)) %>%
  head()
```

## Influence

As noted previously, observations of high leverage may or may not be influential. The influence of an observation depends not only on its leverage, but also on the magnitude of its residual. Recall that while leverage only takes into account the explanatory variable (x), the residual depends on the response variable (y) and the fitted value (y^).

Influential points are likely to have high leverage and deviate from the general relationship between the two variables. We measure influence using Cook's distance, which incorporates both the leverage and residual of each observation.

```{r}
# Rank influential points
mod %>%
augment() %>%
arrange(desc(.cooksd)) %>%
head()
```

## High leverage points

```{r}
# Rank high leverage points
mod %>%
augment()%>%
arrange(desc(.hat),.cooksd)%>%
head()

```

